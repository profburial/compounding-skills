# Command Templates

Full templates for each workflow command generated by `compounding-skills:setup`. All 5 commands are always created. Write them to `.claude/commands/{command_prefix}/`.

## Template Variables

| Variable | Source |
|----------|--------|
| `{command_prefix}` | User-chosen namespace (e.g., "myapp", "venkman") |
| `{project_name}` | Detected project name |
| `{stack}` | Detected framework (e.g., "Ruby on Rails 8.1", "Next.js 14") |
| `{test_command}` | Detected or inferred test runner command |
| `{lint_command}` | Detected or inferred linter command |
| `{plan_dir}` | Plan file directory (default: `docs/plans/`) |
| `{branch_style}` | Git branch naming pattern (e.g., `feat/`, `username/`) |
| `{expert_skill}` | Name of the expert skill (e.g., `expert-rails-developer`) |
| `{skill_file_paths}` | Newline-separated list of all expert skill file paths (see compound template) |

---

## brainstorm Template

**Path:** `.claude/commands/{command_prefix}/brainstorm.md`

```markdown
---
name: {command_prefix}:brainstorm
description: >
  Explore requirements and approaches through collaborative dialogue before planning.
  Answers WHAT to build — precedes {command_prefix}:plan which answers HOW.
argument-hint: [feature idea or problem to explore]
disable-model-invocation: true
---

# {command_prefix}:brainstorm — Collaborative Brainstorming

You are facilitating a brainstorming session to explore **what** to build through collaborative dialogue. This precedes `{command_prefix}:plan`, which answers **how** to build it. **You must never write application code — only explore ideas and capture decisions.**

## Feature Description

<feature_description>$ARGUMENTS</feature_description>

If the feature description above is empty, ask the user: "What would you like to explore? Describe the feature, problem, or improvement you're thinking about."

Do not proceed until you have a feature description.

## Phase 0: Assess Requirements Clarity

Evaluate whether brainstorming is actually needed.

**Clear requirements indicators:**
- Specific acceptance criteria already provided
- References to existing patterns to follow
- Exact expected behavior described
- Well-defined, constrained scope

If the requirements are already clear, use **AskUserQuestion** to suggest: "Your requirements seem detailed enough to skip straight to planning. Should I run `/{command_prefix}:plan` instead, or would you like to explore the idea further?"

If requirements are unclear or the user wants to explore, continue to Phase 1.

## Phase 1: Understand the Idea

### 1.1 Lightweight Repo Research

Launch 1–2 Explore agents to quickly understand existing patterns related to the feature:
- Similar features already in the codebase
- Established conventions and architecture relevant to the idea
- Constraints from CLAUDE.md or existing design decisions

Keep this lightweight — just enough context to ask informed questions.

### 1.2 Collaborative Dialogue

Use **AskUserQuestion** to ask questions **one at a time**. Do not batch multiple questions.

**Question progression — broad to narrow:**
1. **Purpose** — What problem does this solve? Who benefits?
2. **Users** — Who is affected by this?
3. **Scope** — What's the minimum viable version?
4. **Constraints** — Performance requirements, backward compatibility, external dependencies?
5. **Edge cases** — What happens when things go wrong?
6. **Success criteria** — How do we know this works?

**Guidelines:**
- Prefer multiple-choice options when natural choices exist
- Validate your assumptions explicitly ("It sounds like X — is that right?")
- Skip questions the user has already answered in the feature description
- Don't ask more than 5–7 questions total unless the user wants to go deeper

**Exit condition:** Continue until the idea is well-understood OR the user says "proceed."

## Phase 2: Explore Approaches

Propose **2–3 concrete approaches** based on your research and the conversation so far.

For each approach, provide:
- Brief description (2–3 sentences)
- Pros and cons
- When it's best suited

**Lead with your recommendation** and explain why. Apply YAGNI — prefer the simpler solution unless there's a concrete reason for complexity.

Use **AskUserQuestion** to ask which approach the user prefers (or if they have a different idea).

## Phase 3: Capture the Design

Write a brainstorm document to `docs/brainstorms/YYYY-MM-DD-<topic>-brainstorm.md` where:
- `YYYY-MM-DD` is today's date
- `<topic>` is a kebab-cased descriptor

Ensure the `docs/brainstorms/` directory exists before writing (create it if needed).

Use this template:

```markdown
# Brainstorm: <Title>

## What We're Building
What the feature does, who it's for, and why it matters. (2–3 paragraphs max)

## Why This Approach
The chosen approach and why it won over alternatives. Reference the other options considered briefly.

## Key Decisions
- **Decision 1** — Rationale
- **Decision 2** — Rationale

## Scope
What's in scope for the initial implementation. What's explicitly out of scope (for now).

## Open Questions
- Any unresolved questions that need answers before or during planning

## Next Steps
Run `/{command_prefix}:plan` to create an implementation plan from this brainstorm.
```

**Before moving to Phase 4:** If there are Open Questions in the document, ask the user about each one using AskUserQuestion. Move resolved questions to a **Resolved Questions** section. Do not offer to proceed to planning while open questions remain.

## Phase 4: Handoff

Use **AskUserQuestion** to present next steps:

**Question:** "Brainstorm captured. What would you like to do next?"

**Options:**
1. **Review and refine** — Re-read the document together and improve it
2. **Proceed to planning** — Run `/{command_prefix}:plan` with this brainstorm as context
3. **Ask more questions** — Continue exploring before committing
4. **Done for now** — Come back to this later

**If "Proceed to planning":** Tell the user to run `/{command_prefix}:plan` and reference the brainstorm document path.

**If "Review and refine":** Re-read the brainstorm document, identify weak spots or gaps, and suggest improvements. After refining, return to Phase 4.

**If "Ask more questions":** Return to Phase 1.2 and continue the dialogue.

**If "Done for now":** Display the summary below and end.

## Output Summary

When complete, display:

```
Brainstorm complete!

Document: docs/brainstorms/YYYY-MM-DD-<topic>-brainstorm.md

Key decisions:
- [Decision 1]
- [Decision 2]

Next: Run `/{command_prefix}:plan` when ready to create an implementation plan.
```

## Rules

- **Never write application code.** Only the brainstorm document.
- **Focus on WHAT, not HOW.** Implementation details belong in the plan.
- **Ask one question at a time.** Don't overwhelm the user.
- **Apply YAGNI.** Prefer simpler approaches.
- **Keep it concise.** 200–300 words per section max.
```

---

## plan Template

**Path:** `.claude/commands/{command_prefix}/plan.md`

```markdown
---
name: {command_prefix}:plan
description: >
  Create a structured implementation plan for a feature, bug fix, or refactor.
  Researches the codebase first, then produces a plan document — never writes code.
argument-hint: [feature description]
disable-model-invocation: true
---

# {command_prefix}:plan — Implementation Planning

You are creating a structured implementation plan. You will research the codebase thoroughly, then write a plan document. **You must never write application code — only the plan.**

The user's feature description follows this skill prompt as the argument. If no argument was provided, ask the user what they'd like to plan.

## Phase 1: Clarify

Evaluate the feature description. If it's ambiguous or missing critical details, ask 1–3 focused questions using AskUserQuestion before proceeding. Good questions target:

- Which user type or area is affected?
- What's the expected scope — new feature, modification, background job, etc.?
- Are there constraints the user hasn't mentioned (backward compatibility, performance)?

If the description is clear enough to research, skip straight to Phase 2.

## Phase 2: Research

Investigate the codebase before writing anything. Launch 1–3 Explore agents in parallel, focused on:

1. **Existing patterns** — How does the codebase already handle similar functionality? Find concrete examples with file paths and line numbers.
2. **Files that will change** — Which files will need modification or creation?
3. **Related tests** — What test patterns exist for similar features? What test helpers and factories are available?

Search the web when the task involves:
- External APIs or libraries not already used in the codebase
- Security-sensitive patterns (auth flows, encryption, token handling)
- Framework features you can't find evidence of in existing code

After research completes, compile your findings before moving to Phase 3.

## Phase 3: Write the Plan

Write the plan to `{plan_dir}YYYY-MM-DD-<type>-<short-name>-plan.md` where:
- `YYYY-MM-DD` is today's date
- `<type>` is `feature`, `fix`, or `refactor`
- `<short-name>` is a kebab-cased descriptor

Use this template:

```markdown
# <Type>: <Title>

## Context
Why this change is needed — the problem, what prompted it, intended outcome.

## Research Findings
Key discoveries from codebase exploration:
- Existing patterns to follow (with file paths and line numbers)
- Files that will need changes
- Relevant tests that exist

## Approach
Step-by-step implementation plan:
1. Step with file path references
2. ...

Each step should be specific enough that an engineer (or Claude) can execute it without ambiguity.

## Acceptance Criteria
- [ ] Criterion 1
- [ ] Criterion 2

## Risks & Open Questions
(Only if applicable — omit this section entirely if there are none.)

## Verification
How to test the changes end-to-end:
- Commands to run (e.g., `{test_command}`)
- Expected behavior to confirm
```

After writing the plan file, tell the user:
- The path to the plan file
- A brief summary of the approach
- Suggest they review the plan and either proceed with `/{command_prefix}:work` or refine it

## Rules

- **Never write application code.** Only the plan document.
- **Always research before planning.** Don't guess at file paths or patterns — verify them.
- **Reference real files.** Every file path in the plan must come from actual codebase exploration.
- **Scale depth to complexity.** A simple bug fix needs 1 page; a multi-service feature needs more.
- **One plan per invocation.**
```

---

## work Template

**Path:** `.claude/commands/{command_prefix}/work.md`

```markdown
---
name: {command_prefix}:work
description: >
  Execute an implementation plan — reads a plan document from {plan_dir},
  creates tasks, and builds the feature incrementally with tests and linting.
argument-hint: [path to plan file]
---

# {command_prefix}:work — Plan Execution

You are executing a structured implementation plan. You will read a plan document, break it into tasks, and systematically build the feature with tests and linting.

The user may provide a path to a plan file as the argument. If no argument is provided, find the most recently modified `.md` file in `{plan_dir}` and confirm with the user before proceeding.

## Phase 1: Setup

### 1. Read the Plan

Read the plan file. Extract and understand:
- **Context** — What problem are we solving?
- **Approach** — The step-by-step implementation plan (this becomes your task list)
- **Acceptance Criteria** — What "done" looks like
- **Risks & Open Questions** — If this section exists and has unresolved items, ask the user before proceeding
- **Verification** — Commands and expected behavior for end-to-end testing

### 2. Check Git State

Run `git status` and `git branch --show-current`.

- If there are uncommitted changes, warn the user and ask how to proceed.
- If on `main`, offer to create a feature branch. Derive the branch name from the plan's short name (e.g., `{branch_style}feature-name`).

### 3. Create Tasks

Parse the plan's **Approach** section into tasks using TaskCreate. Each numbered step in the plan becomes one task. Include enough detail in each task description that you could execute it without re-reading the plan.

Set up task dependencies with TaskUpdate where steps must be sequential.

### 4. Confirm

Show the user the task list and ask if they want to proceed, adjust anything, or skip certain tasks.

## Phase 2: Execute

Work through tasks in order. For each task:

1. **Mark in-progress** — `TaskUpdate` with `status: "in_progress"`
2. **Implement** — Write the code following `{expert_skill}` conventions
3. **Test** — Run relevant tests: `{test_command}`
4. **Lint** — Run `{lint_command}` on changed files
5. **Simplify** — Spawn the `code-simplifier` agent on the files changed in this task (use `git diff --name-only` to identify them). Wait for completion. If it made changes, re-lint.
6. **Mark completed** — `TaskUpdate` with `status: "completed"`
7. **Next task** — `TaskList` to find the next pending task

### Handling Failures

- **Test failure**: Fix the issue before moving on. Do not mark the task as completed until tests pass.
- **Lint failure**: `{lint_command}` should handle most issues. For remaining ones, fix manually.
- **Blocked**: If you can't proceed without information or a decision, ask the user. Do not guess.
- **Scope creep**: If you discover something that needs doing but isn't in the plan, note it to the user as a follow-up — don't silently expand scope.

## Phase 3: Quality Check

After all tasks are completed:

1. **Run all affected tests together** — run the full relevant test paths
2. **Lint the full changeset** — `{lint_command}` on all changed files (use `git diff --name-only main` to find them)
3. **Verify acceptance criteria** — Go through each criterion from the plan and confirm it's met. Report any gaps.
4. **Run verification commands** — Execute the commands from the plan's Verification section and confirm expected behavior.

Report the results to the user. If everything passes, the feature is ready for review. If there are failures, fix them before declaring done.

## Rules

- **Follow the plan.** The plan is your spec. Don't add features that aren't in the plan.
- **`{expert_skill}` conventions are mandatory.** All code must follow the patterns in that skill.
- **Test everything.** No untested code.
- **Ask, don't guess.** When uncertain about requirements, architecture, or approach — ask.
- **No scope creep.** Suggest improvements as follow-ups after the current work is done.
```

---

## review Template

**Path:** `.claude/commands/{command_prefix}/review.md`

```markdown
---
name: {command_prefix}:review
description: >
  Review a branch against its implementation plan — runs quality checks, inspects diffs,
  and produces a structured report with severity-rated findings. Read-only, never modifies files.
argument-hint: [path to plan file]
disable-model-invocation: true
---

# {command_prefix}:review — Code Review

You are reviewing a branch before merge. You will read the plan, inspect every changed file, run quality checks, and produce a structured report. **You must never modify any application files — this is a read-only review.**

The user may provide a path to a plan file as the argument. If no argument is provided, find the most recently modified `.md` file in `{plan_dir}` and use it. If no plan file exists, skip plan compliance checking (Phase 2).

## Phase 1: Understand Context

### 1. Read the Plan

If a plan file is available, read it and extract:
- **Context** — What problem was being solved?
- **Approach** — What steps were planned?
- **Acceptance Criteria** — What "done" looks like
- **Verification** — How to test the changes

### 2. Identify the Changeset

```bash
git diff main...HEAD --stat
git diff main...HEAD --name-only
git diff --name-only
git diff --cached --name-only
```

Combine all results into the full changeset.

### 3. Read Every Changed File

Read every file in the changeset in full.

## Phase 2: Review Plan vs. Implementation

Skip this phase entirely if no plan file was found.

- Compare the implementation against the plan's **Approach** section
- Check the plan's **Acceptance Criteria** — does the code satisfy each criterion?
- Flag deviations with context

## Phase 3: Run Quality Checks

Run all checks. Do not stop at the first failure — collect all results.

### 1. Targeted Tests

Run specs/tests that correspond to changed files:
```bash
{test_command} [relevant test paths]
```

If a changed file has no corresponding test, flag it as a **P2** finding.

### 2. Linting

Run in read-only mode on changed files only:
```bash
{lint_command_readonly} [changed files]
```

**Never use auto-fix.** This is a read-only review.

### 3. Security

Run any available security scanning tool on the codebase.

Only flag warnings that relate to files in the changeset.

## Phase 4: Inspect the Diff

Review the full diff against `{expert_skill}` conventions. Evaluate each changed file across:

### Correctness
- Logic errors or off-by-one mistakes
- Missing authorization or authentication checks
- Incomplete error handling
- Race conditions in concurrent code

### Conventions
- Code follows `{expert_skill}` patterns
- Naming conventions respected
- Appropriate layer separation

### Security
- Input validation present
- No hardcoded secrets
- Queries properly scoped

### Testing
- Missing spec coverage for new code paths
- Weak assertions
- Missing edge case coverage

## Phase 5: Report

Present findings in this structured format:

---

## Review Report

### Scope
- **Branch**: `<branch-name>`
- **Files changed**: N
- **Plan file**: `<path>` (or "None — ad-hoc review")

### Plan Compliance
| Criterion | Status | Notes |
|-----------|--------|-------|
| Criterion from plan | Met / Unmet / Partial | Details |

### Quality Checks
| Check | Result | Details |
|-------|--------|---------|
| Targeted tests | PASS / FAIL | N specs, N failures |
| Linting | PASS / FAIL | N offenses |

### Findings

#### P1 — Must Fix (blocks merge)
- `file:line` — Description

#### P2 — Should Fix (significant)
- `file:line` — Description

#### P3 — Consider (suggestions)
- `file:line` — Description

### Assessment
**Merge-ready** / **Not merge-ready** — summary sentence.

---

## Rules

- **Never modify files.**
- **Run all checks.**
- **Scope security findings** to the changeset.
- **Include file:line references** for every finding.
```

---

## compound Template

**Path:** `.claude/commands/{command_prefix}/compound.md`

```markdown
---
name: {command_prefix}:compound
description: >
  Extracts knowledge from a work session and updates skill files with new patterns,
  conventions, and techniques as well as creates learning summaries.
  Sits at the end of the workflow pipeline: brainstorm → plan → work → review → compound.
argument-hint: [path to plan file or context]
disable-model-invocation: true
---

# {command_prefix}:compound — Knowledge Extraction

You are extracting knowledge from a completed work session and compounding it into the project's skill files. You will analyze the changeset, compare it against existing documented patterns, and propose updates so that future sessions automatically benefit from what was learned. **You must never modify application code — only skill files, reference files, and learning documents.**

The user may provide a path to a plan file or descriptive context as the argument. If no argument is provided, find the most recently modified `.md` file in `{plan_dir}` and use it.

## Session Context

<session_context>$ARGUMENTS</session_context>

## Phase 1: Gather Session Artifacts

This phase is read-only. Collect everything needed for analysis.

### 1.1 Identify the Changeset

```bash
git log main..HEAD --oneline
git diff main...HEAD --stat
git diff main...HEAD --name-only
git diff --name-only
git diff --cached --name-only
```

### 1.2 Read the Plan File

Find and read the plan file. Extract:
- **What was built** — the feature or fix
- **Approach taken** — key decisions and trade-offs
- **Deviations** — anything that changed from the original plan

### 1.3 Read All Changed Files

Read every file in the changeset in full.

### 1.4 Read Current Skill Files

Read all existing skill and reference files to establish the knowledge baseline:

{skill_file_paths}

## Phase 2: Analyze for New Knowledge

Launch two Explore agents in parallel to analyze the changeset against the knowledge baseline.

### Agent 1 — Pattern Detection

Compare each changed application file against the existing reference files. For each file, determine:

- **Exact match** — follows a documented pattern exactly → skip
- **Pattern evolution** — follows a documented pattern but introduces a meaningful variation → propose an update to the existing section
- **New pattern** — introduces something not covered by any existing reference → propose a new section

Focus on reusable patterns in the layers documented by `{expert_skill}`.

### Agent 2 — Convention & Architecture Detection

Look for changes that affect project-wide conventions:

- New dependencies added
- New external clients or integrations
- New directory structures or namespaces
- New base class usage or shared abstractions
- Changes to authentication or authorization patterns
- New background jobs, channels, or mailers

If this was a bug-fix session, also look for debugging techniques not already captured in `expert-bug-hunter/references/techniques.md`.

## Phase 3: Present Findings & Get Approval

### 3.1 Classify Findings

| Classification | Meaning | Action |
|----------------|---------|--------|
| **New Pattern** | Something genuinely new, not covered by existing docs | Propose new section in appropriate reference file |
| **Pattern Evolution** | A documented pattern used in a new way | Propose addition to existing section |
| **New Convention** | Project-wide convention change | Propose update to SKILL.md or new reference file |
| **Routine Application** | Existing patterns applied correctly | No update needed — skip |

### 3.2 Filter for Signal

A finding must pass ALL:
- **Reusable** — Would this help in future sessions, not just this one?
- **Non-obvious** — Is this something an agent wouldn't infer from existing docs?
- **Stable** — Is this a settled pattern, not an experiment?

If nothing passes these filters, say so and move to Phase 5.

### 3.3 Present the Report

For each proposed update, show:

```
### [Classification]: [Short Title]

**Target file:** `.claude/skills/.../references/xyz.md` (section: "Section Name")
**What's new:** Brief description of the new knowledge
**Source:** `app/path/to/file.rb` (lines N–M)
**Proposed addition:**
> [Preview of what would be added — a few lines max]
```

### 3.4 Get User Approval

Use **AskUserQuestion** to ask:

**Question:** "Which knowledge updates would you like to apply?"

**Options:**
1. **Apply all** — Write all proposed updates to skill files
2. **Select specific** — Choose which updates to apply
3. **Learning doc only** — Skip skill updates, just write the learning document
4. **Skip** — No updates, no learning document

If "Select specific": use a follow-up **AskUserQuestion** with `multiSelect: true` listing each proposed update by title.

## Phase 4: Apply Approved Updates

### Reference Files (`references/*.md`)

**New section:** Append a new H2 section at the end of the file:
- H2 heading with descriptive title
- Annotated code block showing the pattern with real code from the session
- "Key patterns" bullet list explaining what matters and why

**Pattern evolution:** Find the existing H2 section and append to it. Never replace existing content.

### SKILL.md Updates

Add bullet points under the appropriate existing section. Create a new section only if the topic is genuinely distinct.

### New Reference Files

Only create a new reference file when:
- A genuinely new category of code has emerged
- There are at least 2–3 patterns to document
- The user explicitly approves the new file

### Preservation Rule

**Append-only by default.** Never delete, rewrite, or reorganize existing content.

## Phase 5: Write Learning Document

**Always write this file**, regardless of whether skill updates were applied.

Create `docs/learnings/YYYY-MM-DD-<topic>.md` where:
- `YYYY-MM-DD` is today's date
- `<topic>` is a kebab-cased descriptor from the plan title or branch name

Create the `docs/learnings/` directory if it doesn't exist.

```markdown
# Learning: <Title>

**Date:** YYYY-MM-DD
**Branch:** `<branch-name>`
**Plan:** `<path to plan file>` (or "None")

## What Was Built
Brief description. 2–3 sentences.

## Key Decisions
- **Decision 1** — Why this approach was chosen
- **Decision 2** — Trade-offs considered

## New Patterns Introduced
- Pattern name — where it's used, why it matters
(or "None — existing patterns applied correctly")

## Problems Encountered
- Problem description — how it was resolved
(or "None")

## Skill Updates Applied
- `path/to/skill/file.md` — Section: "Section Name" — what was added
(or "None — no new reusable patterns identified")

## Follow-Up Ideas
- Ideas for future improvement
(or "None")
```

## Phase 6: Summary

```
Knowledge extraction complete!

Learning document: docs/learnings/YYYY-MM-DD-<topic>.md

Skill updates applied:
- `.claude/skills/.../references/xyz.md` — Section: "Section Name"
(or "None — no new reusable patterns identified")

Your skills are updated. Future sessions will benefit from these patterns.
```

## Rules

- **Never modify application code.** Only skill files, reference files, and learning documents.
- **Get user approval before every skill update.**
- **Preserve existing skill content.** Append-only.
- **Quality over quantity.** If nothing new was introduced, say so.
- **Match existing tone and format** of the target file.
- **Use real code from the session.** All examples must reference actual files and line numbers.
- **Always write the learning document.**
```

---

## Customization Notes

When generating commands, substitute all `{variable}` placeholders:

1. **`{skill_file_paths}`** in the compound template — replace with actual bullet list of skill files created in Phases 5.2 and 5.3, one per line, formatted as:
   ```
   - `.claude/skills/expert-{stack}-developer/SKILL.md`
   - `.claude/skills/expert-{stack}-developer/references/{layer-1}.md`
   - `.claude/skills/expert-{stack}-developer/references/{layer-2}.md`
   - `.claude/skills/expert-bug-hunter/SKILL.md`
   - `.claude/skills/expert-bug-hunter/references/techniques.md`
   ```
2. **`{lint_command_readonly}`** in the review template — use the read-only variant of the linter (e.g., `standardrb --no-fix`, `eslint` without `--fix`)
3. **Remove or adapt** any section that doesn't apply to the project's stack
4. **Brownfield:** Replace generic path references with actual paths found during Phase 2A analysis
